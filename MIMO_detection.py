#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 21 10:05:19 2019

@author: beck
"""

# LOADED PACKAGES
# Tensorflow/Keras packages
import tensorflow as tf
# from tensorflow import keras
from tensorflow.keras import backend as KB
from tensorflow.keras.optimizers import SGD, Adam, Nadam
# For online training DNN
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate, Reshape, Add
# Python packages
import numpy as np
import time
import yaml
import os

## Own packages
# Include parent folder
import sys
sys.path.append('..')	# Include parent folder, where own packages lie
sys.path.append('.')	# Include current folder, where start simulation script and packages lie
import myfunc as mf
import mymathops as mop
import myequ as equ
import mycom as com
import mytraining as mt
import mymimoch as mch



# Functions exclusive to this file

class CMD_graph():
    '''CMDNet graph object
    Original implementation from:
    Beck, E.; Bockelmann, C.; Dekorsy, A. CMDNet: Learning a Probabilistic Relaxation of
    Discrete Variables for Soft Detection with Low Complexity. IEEE Trans. Commun. 2021, 69, 8214-8227
    '''
    # Class Attribute
    name = 'Tensorflow CMDNet graph object'
    # Initializer / Instance Attributes
    def __init__(self, train_par, delta0, taui0, soft = 0, multiloss = 0, binopt = 0):
        '''CMDNet Initialization
        binopt: Version of CMDNet for two-classes/binary decision with BPSK or QPSK (CMDNet_bin)
        '''
        self.inputs = 0
        self.train_inputs = 0
        self.outputs = 0
        self.loss = 0
        self.params = 0
        if train_par.mod.M == 2 and binopt == 1:
            self.create_graph_bin(train_par, delta0, taui0, soft = soft, multiloss = multiloss)
        else:
            self.create_graph(train_par, delta0, taui0, soft = soft, multiloss = multiloss)
        self.predict = KB.function(self.inputs, self.outputs)           # Calculate Output (Soft information/symbols) given Input
        # self.cross_entr = KB.function(self.train_inputs, self.loss)   # Loss output
    # Instance methods
    def create_graph_bin(self, train_par, delta0, taui0, soft = 0, multiloss = 0):
        '''Binary concrete MAP Detection Unfolding - CMDNet bin
        Create graph with variables and placeholders
        train_par: Training parameters object
        delta0: Step size starting point
        taui0: Inverse softmax temperature starting point
        soft: MSE: = 1 | Binary_Cross_Entropy: = 0
        multiloss: Multiloss on: = 1
        '''
        [Nr, Nt, it] = [train_par.Nr, train_par.Nt, train_par.L]
        # Select alpha according to paper notation m = [-1, 1]
        if (train_par.mod.m == np.array([-1, 1])).all():
            alpha = train_par.mod.alpha[:, 0]
        else:
            alpha = train_par.mod.alpha[:, 1]

        # CMD Unfolding
        # Create graph with variables and placeholders
        yt = KB.placeholder(name="yt", shape=(None, Nr))
        Ht = KB.placeholder(name="Ht", shape=(None, Nr, Nt))
        sigmat0 = KB.placeholder(name="sigmat", shape=(None, ))
        if soft == 1:
            out_true = KB.placeholder(name="out_true", shape=(None, Nt))
        else:
            out_true = KB.placeholder(name="out_true", shape=(None, Nt, 2)) # two classes

        # Inputs
        self.train_inputs = [yt, Ht, sigmat0, out_true]
        self.inputs = self.train_inputs[0:-1]
        # Variables
        s0 = KB.variable(value = np.zeros((Nt)))
        taui = KB.variable(value = taui0)
        delta = KB.variable(value = delta0)
        self.params = [delta, taui]
        # Loss
        loss = []

        # Preprocessing
        alphat = KB.constant(value = alpha)
        sigmat = KB.expand_dims(sigmat0, axis = -1)
        HH = KB.batch_dot(KB.permute_dimensions(Ht, (0, 2, 1)), Ht)
        yH = KB.batch_dot(yt, Ht)
        
        # UNFOLDING / Starting point of first layer
        s = KB.transpose(KB.expand_dims(s0)) * KB.ones_like(Ht[:, 0, :])
        taui_abs = KB.abs(taui[0])
        xt = KB.tanh((KB.log(1 / alphat - 1) + s) / 2 * taui_abs)
        
        for iteration in range(0, it):
            xHH = KB.batch_dot(xt, HH)
            grad_x = 1 / 2 * taui_abs * (1 - xt ** 2) 
            grad_L = sigmat ** 2 * KB.tanh(s / 2) + grad_x * (xHH - yH)
            # grad_L = KB.tanh(s / 2) + 1 / sigmat ** 2 * grad_x * (xHH - yH) # original version
            # Gradient/ResNet Layer
            s = s - delta[iteration] * grad_L
            
            # Start of new gradient descent iteration
            taui_abs = KB.abs(taui[iteration + 1]) # no negative values for tau !
            xt = KB.tanh((KB.log(1 / alphat - 1) + s) / 2 * taui_abs)
            xt2 = KB.expand_dims(xt, axis = -1)
            if (train_par.mod.m == np.array([-1, 1])).all():
                ft = KB.concatenate([(1 - xt2) / 2, (1 + xt2) / 2], axis = -1) # [q(x = -1), q(x = 1)]
            else:
                ft = KB.concatenate([(1 + xt2) / 2, (1 - xt2) / 2], axis = -1) # [q(x = 1), q(x = -1)]
            if multiloss == 1:
                if soft == 1:
                    # 2. mean should be a sum for overall MSE scaling with Nt
                    lloss = (iteration + 1) * KB.mean(KB.mean((out_true - xt) ** 2, axis = -1))
                else:
                    # 2. mean should be a sum since q factorizes
                    lloss = (iteration + 1) * KB.mean(KB.mean(KB.categorical_crossentropy(out_true, ft, axis = -1), axis = -1))
                loss.append(lloss)
        
        
        if multiloss == 1:
            loss = KB.sum(loss)
        else:
            # Output layer and objective function
            if soft == 1:
                # 2. mean should be a sum for overall MSE scaling with Nt
                loss = KB.mean(KB.mean((out_true - xt) ** 2, axis = -1))
            else:
                # 2. mean should be a sum since q factorizes
                loss = KB.mean(KB.mean(KB.categorical_crossentropy(out_true, ft, axis = -1), axis = -1))
        self.loss = loss
        self.outputs = [ft, xt]
        return self.train_inputs, self.outputs, self.loss, self.params
    
    def create_graph(self, train_par, delta0, taui0, soft = 0, multiloss = 0):
        '''Concrete MAP Detection Unfolding - CMDNet
        Create graph with variables and placeholders
        train_par: Training parameters object
        delta0: Step size starting point
        taui0: Inverse softmax temperature starting point
        soft: MSE: = 1 | Binary_Cross_Entropy: = 0
        multiloss:  Multiloss on: = 1
        '''
        ce = 0  # Truncated or exact softmax implementation? 0/1
        [Nr, Nt, it, mod, alpha] = [train_par.Nr, train_par.Nt, train_par.L, train_par.mod, train_par.mod.alpha]
        N_class = mod.M

        # CMD Unfolding - CMDNet
        # Create graph with variables and placeholders
        yt = KB.placeholder(name="yt", shape=(None, Nr))
        Ht = KB.placeholder(name="Ht", shape=(None, Nr, Nt))
        sigmat0 = KB.placeholder(name="sigmat", shape=(None, ))
        if soft == 1:
            out_true = KB.placeholder(name="out_true", shape=(None, Nt))
        else:
            out_true = KB.placeholder(name="out_true", shape=(None, Nt, N_class))
        
        # Inputs
        self.train_inputs = [yt, Ht, sigmat0, out_true]
        self.inputs = self.train_inputs[0:-1]
        # Variables
        G0 = KB.variable(value = np.zeros((Nt, N_class)))
        taui = KB.variable(value = taui0)
        delta = KB.variable(value = delta0)
        self.params = [delta, taui]
        # Loss
        loss = []

        # Preprocessing
        alphat = KB.constant(value = alpha)
        m = KB.expand_dims(KB.expand_dims(KB.variable(value = mod.m), axis = 0), axis = 0)
        sigmat = KB.expand_dims(KB.expand_dims(sigmat0, axis = -1), axis = -1)
        HH = KB.batch_dot(KB.permute_dimensions(Ht, (0, 2, 1)), Ht)
        yH = KB.batch_dot(yt, Ht)
        
        # UNFOLDING / Starting point of first layer
        G = KB.expand_dims(G0, axis = 0) * KB.expand_dims(KB.ones_like(Ht[:, 0, :]), axis = -1)
        taui_abs = KB.abs(taui[0])
        ft = KB.softmax((KB.log(alphat) + G) * taui_abs, axis = -1)
        xt = KB.sum(ft * m, axis = -1)

        for iteration in range(0, it):
            xHH = KB.batch_dot(xt, HH)
            grad_x = taui_abs * (ft * m - ft * KB.expand_dims(xt, axis = -1))
            grad_L = sigmat ** 2 * (1 - KB.exp(-G)) + grad_x * KB.expand_dims(xHH - yH, axis = -1)          # Implemented version from article
            # grad_L =  (1 - KB.exp(-G)) + 1 / sigmat ** 2 * grad_x * KB.expand_dims(xHH - yH, axis = -1)   # Original version from article
            # Gradient/ResNet Layer
            G = G - delta[iteration] * grad_L
            # Start of new gradient descent iteration
            taui_abs = KB.abs(taui[iteration + 1])      # -> No negative values for tau !
            logits = (KB.log(alphat) + G) * taui_abs
            ft = KB.softmax((KB.log(alphat) + G) * taui_abs, axis = -1)
            xt = KB.sum(ft * m, axis = -1)
            if multiloss == 1:
                if soft == 1:
                    # 2. mean should be a sum for overall MSE scaling with Nt
                    lloss = (iteration + 1) * KB.mean(KB.mean((out_true - xt) ** 2, axis = -1))
                else:
                    # 2. mean should be a sum since q factorizes
                    if ce == 0:
                        lloss = (iteration + 1) * KB.mean(KB.mean(KB.categorical_crossentropy(out_true, ft, axis = -1, from_logits = False), axis = -1)) # -(-ft) for truncated cross entropy with faster training convergence
                    else:
                        lloss = (iteration + 1) * KB.mean(KB.mean(KB.categorical_crossentropy(out_true, logits, axis = -1, from_logits = True), axis = -1))
                loss.append(lloss)
        
        
        if multiloss == 1:
            loss = KB.sum(loss)
        else:
            # Output layer and objective function
            if soft == 1:
                # 2. mean should be a sum for overall MSE scaling with Nt
                loss = KB.mean(KB.mean((out_true - xt) ** 2, axis = -1))
            else:
                # 2. mean should be a sum since q factorizes
                if ce == 0:
                    loss = KB.mean(KB.mean(KB.categorical_crossentropy(out_true, ft, axis = -1, from_logits = False), axis = -1)) # -(-ft) for truncated cross entropy with faster training convergence
                else:
                    loss = KB.mean(KB.mean(KB.categorical_crossentropy(out_true, logits, axis = -1, from_logits = True), axis = -1))
        
        self.loss = loss
        self.outputs = [ft, xt]

        return self.train_inputs, self.outputs, self.loss, self.params




def train(Nepoch, train_hist, inputs, loss, params, opt, data_gen, saveobj, it_checkp = 100, sv_checkp = 0, sel_best_weights = 0, esteps = 0, lr_dyn = 0):
    '''Training in TensorFlow 1 with Keras Backend
    INPUT
    Nepoch: Number of training iterations
    train_hist: Object with history of training and validation loss, etc.
    inputs: Inputs of graph
    loss: Loss of optimization problem
    params: Parameter in graph to be trained
    opt: Optmizer (keras)
    data_gen: Data generator / data_gen.data_val: includes Validation data
    saveobj: Object with saving functionality
    add_params: Additional parameters that are fixed, but to be stored
    OPTIONAL
    it_checkp: Iterations until validation checkpoint
    sv_checkp: If model is saved at checkpoint
    sel_best_weights: If best weights are selected in the end
    esteps: Iterations w/o improvement until early stopping
    lr_dyn: Learning rate dynamic according to function lr_schedule()
    OUTPUT
    train_hist: Object with history of training and validation loss, etc.
    '''
    updates = opt.get_updates(params = params, loss = loss)
    train = KB.function(inputs, [loss], updates = updates)
    test = KB.function(inputs, [loss])
    # History object 
    if train_hist.epoch == []:
        # First evaluation, if empty
        start_time = time.time()
        val_loss = test(data_gen.data_val)[0]
        train_hist(0, 0, val_loss, params, opt, time.time() - start_time) # no train_loss
        train_hist.printh()
    else:
        # Initialization from train_history -> TODO: Move higher ? -> e.g., self.sel_epoch = 5 -> erase content afterwards in all lists???
        # idea epoch_reset: define function in train_hist resetting after sel_epoch
        train_hist.sel_weights(params, -1) # KB.batch_set_value(list(zip(params, train_hist.params[-1])))
        opt.from_config(train_hist.opt_config[-1])
        opt.set_weights(train_hist.opt_params[-1])
    if Nepoch != 0:
        start_time = time.time()
        for epoch in range(train_hist.epoch[-1], Nepoch):
            # Create new training data set every batch -> only 1 epoch
            data_gen()
            train_loss = train(data_gen.data)[0]
            if np.isnan(train_loss):
                print('Encountered NaN in training loss: Stopped training.')
                break
            # Validation after it_checkp steps / Last evaluation is saved for sure
            if (epoch + 1) % it_checkp == 0 or epoch + 1 == Nepoch:
                val_loss = test(data_gen.data_val)[0] # val_loss = test(data_val)[0]
                train_hist(epoch + 1, train_loss, val_loss, params, opt, time.time() - start_time, add_params = data_gen.data_opt) # tracking loop time w/o overhead
                if sv_checkp == 1 or epoch + 1 >= Nepoch: # Save if checkpoint or last iteration
                    saveobj.save(train_hist.filename, train_hist.obj2dict())
                train_hist.printh()
                # Early Stopping / Decreasing learning rate schedule
                if esteps > 0 and train_hist.early_stopping(esteps):
                    if lr_dyn == 1:
                        # learning rate schedule, e.g., here exponential
                        func_lr = lr_schedule(train_hist.opt_config[-1]['learning_rate'])
                        KB.set_value(opt.lr, func_lr)
                        print('Decreased learning rate.')
                    else:
                        print('Early stopping...')
                        saveobj.save(train_hist.filename, train_hist.obj2dict()) # save if early stopping
                        break
                start_time = time.time()

    # Select best weights
    if sel_best_weights == 1:
        train_hist.set_best_weights(params)
    return train_hist


def lr_schedule(lr):
    '''Computes learning rate schedule
    lr: learning rate
    lr_new: New learning rate after update step
    '''
    lr_new = lr / 2
    return lr_new


def CMD_initpar(M, L, typ, k = 0):
    '''Calculate a good heuristic starting point for CMD / loading starting point
    M: Number of classes / Modulation order
    L: Number of layers / iterations
    'typ': linear/constant
    k: Number of starting points, e.g., for CMDpar
    delta0: Starting point of iterative step sizes
    taui0: Starting point of iterative softmax temperatures
    '''
    # Finetuning of starting point has to be done at this point
    # Maximum parameter values
    if M == 4:
        tau_max = 1 / (M - 1) * 2   # NOTE: * 2 shows better results for 4ASK or 16QAM
    else:
        # Default softmax temperature that makes the concrete distribution convex instead of concave
        # and thus gives better approximation of discrete random variables
        tau_max = 1 / (M - 1)
    delta_max = 1
    # Minimum parameter values
    tau_min = 0.1                   # * tau_max # default: 0.1
    delta_min = 0.1                 # * delta_max # default: 0.1
    if  typ.casefold() == 'linear':
        # Linear decrease
        tau0 =  tau_max - (tau_max - tau_min) / L * np.linspace(0, L, L + 1)
        delta0 = delta_max - (delta_max - delta_min) / L * np.linspace(0, L, L)
        taui0 = 1 / tau0
    elif typ.casefold() == 'const':
        # Constant
        tau0 = tau_max * np.ones(L + 1)
        delta0 = delta_max  * np.ones(L)
        taui0 = 1 / tau0
    elif typ.casefold() == 'load':
        ## Load starting point
        # Choose it here in the code
        ## MIMO 64x64 (default)
        sim_set = {
            'Mod': 'QPSK',
            'Nr': 64,
            'Nt': 64,
            'L': 64,
        }
        fn2 = mf.filename_module('trainhist_', 'curves', 'CMD', '_binary_tau0.1', sim_set) # default: _binary_tau0.1
        ## MIMO 16x16
        # sim_set = {
        #     'Mod': 'QPSK',
        #     'Nr': 16,
        #     'Nt': 16,
        #     'L': 16,
        # }
        # fn2 = mf.filename_module('trainhist_', 'curves', 'CMD', '_binary', sim_set) # default: _binary
        ##
        saveobj2 = mf.savemodule('npz')
        train_hist2 = mt.training_history()
        train_hist2.dict2obj(saveobj2.load(fn2.pathfile))
        [delta0, taui0] = train_hist2.params[-1]
    else:
        # Default: Only linear decrease in taui, delta constant
        tau0 =  tau_max - (tau_max - tau_min) / L * np.linspace(0, L, L + 1)
        delta0 = delta_max  * np.ones(L)
        taui0 = 1 / tau0
    
    if k != 0:
        # For more than one starting point, e.g., CMDpar
        tau0 = tau0[:, np.newaxis].repeat((k), axis = -1)
        delta0 = delta0[:, np.newaxis].repeat((k), axis = -1)
        taui0 = 1 / tau0

    return delta0, taui0








class DetNet_graph():
    '''DetNet graph object
    For loading a DetNet graph from original source code and evaluating it inside this script
    '''
    # Class Attribute
    name = 'Tensorflow DetNet graph object'
    # Initializer / Instance Attributes
    def __init__(self, train_par, fn_ext):
        self.inputs = 0
        self.outputs = 0
        self.graph = 0
        self.create_graph(train_par, fn_ext)
        with self.graph.as_default():
            session_one = tf.Session()
            with session_one.as_default():
                self.predict = KB.function(self.inputs, self.outputs) # Output soft information/symbols
    # Instance methods
    def generate_filename(self, train_par, fn_ext):
        '''Generate filename for DetNet load
        train_par: Training parameter object
        fn_ext: Filename extension
        '''
        filename = 'DetNet_' + train_par.mod.mod_name + '_{}_{}_{}_snr{}_{}'.format(train_par.Nt, train_par.Nr, train_par.L, train_par.EbN0_range[0], train_par.EbN0_range[1]) + fn_ext
        path = os.path.join('models', train_par.mod.mod_name, '{}x{}'.format(train_par.Nt, train_par.Nr))
        path2 = os.path.join(path, filename)
        pathfile = os.path.join(path2, filename)
        return pathfile, path2
    def create_graph(self, train_par, fn_ext):
        '''Load and create DetNet graph from file
        train_par: Training parameter object
        fn_ext: Filename extension
        '''
        # Reload the model/graph into the session
        pathfile, path = self.generate_filename(train_par, fn_ext)
        new_saver = tf.train.import_meta_graph(pathfile + '.meta')
        # Workaround: Use global session...
        new_saver.restore(KB.get_session(), tf.train.latest_checkpoint(path))
        graph = tf.get_default_graph()
        
        # Collect inputs and outputs
        # Inputs
        HY = graph.get_tensor_by_name("HY:0")
        HH = graph.get_tensor_by_name("HH:0")
        # X = graph.get_tensor_by_name("X:0")
        # X_IND = graph.get_tensor_by_name("X_IND:0")
        self.inputs = [HY, HH]
        # Outputs
        S1 = graph.get_tensor_by_name("S1_" + str(train_par.L - 1) + ":0")
        # Reshape for QAM16
        if train_par.mod.M == 4:
            # # Reshape soft output acc. to script notation
            # S2 = tf.transpose(S2, perm = [0, 3, 1, 2])
            S3 = graph.get_tensor_by_name("S3_" + str(train_par.L - 2) + ":0")
            S3 = S3 / tf.expand_dims(tf.reduce_sum(S3, axis = -1), axis = -1)
            S3 = tf.expand_dims(S3, axis = -1)
            S4_re = tf.concat([ S3[:, :, 0] + S3[:, :, 1] + S3[:, :, 2] + S3[:, :, 3],
                                S3[:, :, 4] + S3[:, :, 5] + S3[:, :, 6] + S3[:, :, 7],
                                S3[:, :, 8] + S3[:, :, 9] + S3[:, :, 10] + S3[:, :, 11],
                                S3[:, :, 12] + S3[:, :, 13] + S3[:, :, 14] + S3[:, :, 15]]
                                , axis = -1)
            S4_re = S4_re / tf.expand_dims(tf.reduce_sum(S4_re, axis = -1), axis = -1)
            S4_im = tf.concat([ S3[:, :, 0] + S3[:, :, 4] + S3[:, :, 8] + S3[:, :, 12],
                                S3[:, :, 1] + S3[:, :, 5] + S3[:, :, 9] + S3[:, :, 13],
                                S3[:, :, 2] + S3[:, :, 6] + S3[:, :, 10] + S3[:, :, 14],
                                S3[:, :, 3] + S3[:, :, 7] + S3[:, :, 11] + S3[:, :, 15]]
                                , axis = -1)
            S4_im = S4_im / tf.expand_dims(tf.reduce_sum(S4_im, axis = -1), axis = -1)        
            S4 = tf.concat([S4_re, S4_im], axis = 1)
            # # switch output order from -3,-1,1,3 to [-3,-1,3,1]
            perm = [0, 1, 3, 2]
            out = tf.gather(S4, perm, axis = -1)    
        else:
            # Own modifications to output to ensure valid pmf
            S2 = graph.get_tensor_by_name("Out_" + str(train_par.L - 1) + ":0")
            S2_clip = tf.clip_by_value(S2, 0, 1)
            S2t = tf.reshape(S2_clip, (-1, train_par.Nt, train_par.mod.M))
            # out = S2t
            out = S2t / tf.expand_dims(tf.reduce_sum(S2t, axis = -1), axis = -1)
        # SER = graph.get_tensor_by_name("SER:0")
        self.outputs = [out, S1]
        ## Original output (BPSK)
        # last_layer = np.array(sess.run([S2], {HY: batch_HY, HH: batch_HH, X: batch_X, X_IND:BATCH_X_IND}))
        # last_layer = np.clip(last_layer, -1, 1)
        # ind1_last_layer = last_layer[:, 0:2*sim_params.Nt:2]
        # ind2_last_layer = last_layer[:, 1:2*sim_params.Nt:2]
        # fr = ind1_last_layer
        self.graph = graph
        return self.inputs, self.outputs, self.graph




class MMNet_graph():
    '''MMNet graph object
    For loading a MMNet graph from original source code and evaluating it inside this script
    '''
    # Class Attribute
    name = 'Tensorflow MMNet graph object'
    # Initializer / Instance Attributes
    def __init__(self, train_par, algo, fn_ext):
        self.inputs = 0
        self.outputs = 0
        self.create_graph(train_par, algo, fn_ext)
        self.predict = KB.function(self.inputs, self.outputs) # Calculate Output (Soft information/symbols) given Input
    # Instance methods
    def generate_filename(self, train_par, algo, fn_ext):
        '''Generate filename for MMNet load
        algo: Algorithm name; see MMNet implementation
        train_par: Training parameter object
        fn_ext: Filename extension
        '''
        filename = algo + '_' + train_par.mod.mod_name + '_{}_{}_{}_snr{}_{}'.format(train_par.Nt, train_par.Nr, train_par.L, train_par.EbN0_range[0], train_par.EbN0_range[1]) + fn_ext
        path = os.path.join('models', train_par.mod.mod_name, '{}x{}'.format(train_par.Nt, train_par.Nr))
        path2 = os.path.join(path, filename)
        pathfile = os.path.join(path2, filename)
        return pathfile, path2
    def create_graph(self, train_par, algo, fn_ext):
        '''Load and create MMNet graph from file
        algo: Algorithm name; see MMNet implementation
        train_par: Training parameter object
        fn_ext: Filename extension
        OUTPUT
        inputs = [snr_db_max, snr_db_min, batch_size, H]
        outputs = [fr, z, x_out, x, constellation]
        TODO: Noise variance tensor is missing for soft output (LLR) calculation (use with channel coding)
        '''
        pathfile, path = self.generate_filename(train_par, algo, fn_ext)
        new_saver = tf.train.import_meta_graph(pathfile + '.meta')
        # Workaround: Use global session...
        new_saver.restore(KB.get_session(), tf.train.latest_checkpoint(path))
        graph = tf.get_default_graph()
        
        # Collect inputs and outputs
        # Inputs
        H = graph.get_tensor_by_name("H:0")
        snr_db_max = graph.get_tensor_by_name("snr_db_max:0")
        snr_db_min = graph.get_tensor_by_name("snr_db_min:0")
        batch_size = graph.get_tensor_by_name("batch_size:0")
        #lr = graph.get_tensor_by_name("lr:0")
        #train_flag = graph.get_tensor_by_name("train_flag:0")
        self.inputs = [snr_db_max, snr_db_min, batch_size, H]                       #, batch_size, lr, train_flag]
        # Outputs
        x_NN = graph.get_tensor_by_name("Out:0") * tf.math.sqrt(2.0)                # sqrt(2) for consistency with implementation
        x_out = x_NN[-1, :, :]
        x = graph.get_tensor_by_name("input:0") * tf.math.sqrt(2.0)                 # sqrt(2) for consistency with implementation
        constellation = graph.get_tensor_by_name("const:0") * tf.math.sqrt(2.0)     # sqrt(2) for consistency with implementation
        indices = graph.get_tensor_by_name("indices:0")
        z = tf.one_hot(indices, depth = train_par.mod.M, dtype = tf.int32)
        # Reshape for QAM16
        if train_par.mod.M == 4:
            constellation = tf.gather(constellation, [0, 1, 3, 2])
            z = tf.gather(z, [0, 1, 3, 2], axis = -1)
        elif train_par.mod.M == 16:
            print('Not yet implemented!!!')
        # Return class in addition to symbols
        clr = tf.argmin(tf.abs(tf.expand_dims(x_out, axis = -1) - tf.expand_dims(tf.expand_dims(constellation, axis = 0), axis = 0)) ** 2, axis = -1)
        fr = tf.one_hot(clr, depth = train_par.mod.M, dtype = tf.int32)
        self.outputs = [fr, z, x_out, x, constellation]
        ## Numpy equivalent
        # z = keras.utils.to_categorical(indices, num_classes = sim_params.mod.M).astype('int32')
        # # Compute output
        # clr = np.argmin(np.abs(s[:, :, np.newaxis] - constellation[np.newaxis, np.newaxis, :]) ** 2, axis = -1)
        # fr = keras.utils.to_categorical(clr, num_classes = sim_params.mod.M).astype('int32')
        return self.inputs, self.outputs



def MIMODNN(sim_par, Nh, NL = 1, soft = 0):
    '''Simple DNN Detector/Classifier with residual connections for MIMO Equalization
    INPUT
    sim_par: simulation parameter object
    Nh: Intermediate layer width
    NL: Number of indermediate layers
    soft: hard output with probability estimate (0: default CE loss) / soft output with estimate of x (1: MSE loss)
    (Nrx: Number of receive antennas)
    (M: Number of classes/constellation symbols)
    (Ntx: Number of transmit antennas)
    OUTPUT
	dnnmimo: DNN model
    '''
    Nrx = sim_par.Nr
    M = sim_par.mod.M
    Ntx = sim_par.Nt
    inputs = Input(shape = (Nrx, ))
    layer = inputs
    
    # Intermediate layers
    for indL in range(0, NL):
        layer_prev = layer
        if indL == 0 and Nh != Nrx:
            # If input and layer dimension do not match, introduce a linear layer to the first residual connection
            layer_prev = Dense(Nh, activation = 'linear')(layer_prev)
        layer = Dense(Nh, activation = 'relu', kernel_initializer = 'he_uniform')(layer_prev)
        # Residual connection improves training speed/convergence and expressive power
        layer = Add()([layer_prev, layer])
    
    if soft == 1:
        # Final linear layer for estimation
        outputs = Dense(Ntx, activation = 'linear')(layer)
        loss = 'mean_squared_error'
    else:
        # Final softmax layer for classification
        softmax = []
        for _ in range(0, Ntx):
            layer_last = Dense(M, activation = 'softmax')(layer)
            layer_last2 = Reshape((1, M))(layer_last)
            softmax.append(layer_last2)
        outputs = Concatenate(axis = 1)(softmax)
        loss = 'categorical_crossentropy'
    
    dnnmimo = Model(inputs = inputs, outputs = outputs)
    return dnnmimo, loss


def opt_sel(opt_name = 'sgd', lrs = None):
    '''Selection of an Keras optimizer
    INPUT
    opt_name: Optimizer as string
    lrs: Learning rate schedule (optional)
    OUTPUT
    opt: Optimizer object from Keras
    '''
    if lrs == None:
        if opt_name.casefold() == 'adam':
            opt = Adam()                                                        # NOTE: learning_rate = 10**-4 (HyperCMDfull)
        elif opt_name.casefold() == 'sgd':
            opt = SGD(momentum = 0.9, nesterov = True)                          # lr = 1e-6/2, momentum = 0.9, nesterov = True
        elif opt_name.casefold() == 'nadam':
            opt =  Nadam()
        else:
            print('Optimizer not available!')
    else:
        if opt_name.casefold() == 'adam':
            opt = Adam(learning_rate = lrs)                                     # NOTE: learning_rate = 10**-4 (HyperCMDfull)
        elif opt_name.casefold() == 'sgd':
            opt = SGD(learning_rate = lrs, momentum = 0.9, nesterov = True)     # lr = 1e-6/2, momentum = 0.9, nesterov = True
        elif opt_name.casefold() == 'nadam':
            opt =  Nadam(learning_rate = lrs)
        else:
            print('Optimizer not available!')
    return opt


def data_gen_mimo(data_generator):
    '''Data generator for memory-efficient DNN online learning
    with huge datasets; comes at the expense of slower training
    INPUT
    data_generator: Model data generator
    OUTPUT
    data[0], data[-1]: Received signal and chosen output class or symbol as tuple
    '''
    while True:
        data = data_generator.gen_online_mimodata()
        yield (data[0], data[-1])

class data_gen_mimo2():
    '''Data generator data_gen_mimo as object
    Used for debugging as it counts the number of execution iterations
    '''
    name = 'Data generator object'
    def __init__(self):
        self.it = 0
    def __call__(self, data_generator):
        while True:
            data = data_generator.gen_online_mimodata()
            self.it = self.it + 1
            yield (data[0], data[-1])



if __name__ == '__main__':
#     my_func_main()
# def my_func_main():

    # Set working directory to script directory
    path0 = os.path.dirname(os.path.abspath(__file__))	# Path of script being executed
    os.chdir(path0)                                     
    

    # Load settings
    settings_file = "sim_settings.yaml"         # sim_settings.yaml, sim_settings_llr.yaml, sim_settings_online.yaml

    with open(settings_file, 'r') as stream:
        set_dict = yaml.safe_load(stream)
    print('"' + settings_file + '" loaded.')    # Settings file already loaded? Can be changed then...
    load_set = set_dict['load_set']
    llr_mode = load_set['sim']                  # 0/1: BER evaluation, 2: LLR evaluation
    sim_set = set_dict['sim_set']
    code_set = set_dict['code_set']
    train_set = set_dict['train_set']


    # Initialization
    KB.clear_session()                  # clearing graphs
    # np.random.seed(0)                 # predictable random numbers for debugging
    np.random.seed()                    # random seed in every run
    KB.set_floatx(load_set['prec'])     # computation accuracy: 'float16', 'float32', or 'float64'
    if load_set['gpu'] == 0:
        mt.tf_enable_GPU(0, 8)          # disable GPU
        # mt.tf_enable_GPU(1, 1)        # enable GPU / default


    # Create simulation objects
    mod = com.modulation(sim_set['Mod'])
    # Parameters: Nt, Nr, L, mod, N_batch, EbN0
    sim_params = mf.simulation_parameters(sim_set['Nt'], sim_set['Nr'], sim_set['L'], mod, sim_set['Nbatch'], sim_set['ebn0_range'], sim_set['rho'])
    sim_params.mod.alpha = 1 / sim_params.mod.M * np.ones((sim_params.Nt, sim_params.mod.M))  # Optional input # TODO: Dimensions fit w/o input?
    sim_params.snr_gridcalc(sim_set['snr_grid'])
    train_params = mf.simulation_parameters(sim_params.Nt, sim_params.Nr, sim_params.L, sim_params.mod, train_set['batch_size'], train_set['ebn0_range'], sim_params.rho)
    
    # Create save object
    saveobj = mf.savemodule(load_set['sv_ext'])
    # Load code matrices
    codeon = (code_set['code'] != 'uncoded')
    if codeon:
        code = saveobj.load(os.path.join(code_set['path'], code_set['code']))
        G = code['G']
        H = code['H']
    # Create filename object -> for training without code parameters (TODO: maybe train with code)
    fn = mf.filename_module(load_set['fn_train'], sim_set['path'], sim_set['algo'], sim_set['fn_ext'], sim_set)

    
    # TRAINING/ALGORITHM INITIALIZATION
    ## Load DetNet
    if sim_set['algo'] == 'DetNet':
        detnet = DetNet_graph(train_params, sim_set['fn_ext'])
    
    if llr_mode == 2:
        ## LLR comparison
        ## Load DetNet [fixed comparison to this version of DetNet]
        # if sim_set['algo'] == 'DetNet':
        train_ebn0_detnet = [4, 11]
        fn_ext_detnet = '_defsnr'
        train_params2 = mf.simulation_parameters(sim_params.Nt, sim_params.Nr, sim_params.L, sim_params.mod, train_set['batch_size'], train_ebn0_detnet, sim_params.rho)
        detnet_llr = DetNet_graph(train_params2, fn_ext_detnet)

    ## Load MMNet or OAMPNet
    if sim_set['algo'] in ['MMNet', 'OAMPNet']:
        mmnet = MMNet_graph(train_params, sim_set['algo'], sim_set['fn_ext'])


    # CMDNet/CMD
    if sim_set['algo'] == 'CMD_fixed':
        # Choose parameters / starting point
        if load_set['train'] == 1:
            fn2 = mf.filename_module('trainhist_', 'curves', 'CMD', '_binary_splin', sim_set)
            train_hist = mt.training_history()
            train_hist.filename = fn2.pathfile
            train_hist.dict2obj(saveobj.load(train_hist.filename))
            # [delta0, taui0], _ = train_hist.sel_best_weights()
            [delta0, taui0] = train_hist.params[-1]
        else:
            [delta0, taui0] = CMD_initpar(sim_params.mod.M, sim_params.L, train_set['start_point'])

    cmd_algos = ['CMD', 'CMDNet', 'HyperCMD', 'CMDpar']     # CMDNet approaches
    
    if sim_set['algo'] in cmd_algos + ['DNN']:
        # Prepare data
        data_gen = mch.dataset_gen(train_params, soft = train_set['soft'], online = train_set['online'])
        data_gen.N_bval = train_set['val_size']
        data_gen.data_val = data_gen.gen_mimodata(data_gen.N_bval).copy()
        
        # Online learning: Check if last online learning iteration equals an iteration checkpoint -> last iteration needs to be saved
        if (train_set['online'] != 0) and (train_set['online'] % train_set['it_checkp'] != 0):
            train_set['it_checkp'] = train_set['online']
        
        # Initialize optimizer
        if train_set['lr_dyn']['mode'] == 2:
            # Dynamic learning rate schedule, piecwise constant decay
            # DetNet FullyCon: initial lr=0.0008, then decay by factor 0.97 after 1000 iterations, 1e6 iterations in total
            lrs = tf.keras.optimizers.schedules.PiecewiseConstantDecay(train_set['lr_dyn']['boundaries'], train_set['lr_dyn']['values'])
        else:
            # Default learning rate
            lrs = None
        opt = opt_sel(opt_name = train_set['opt'], lrs = lrs)


    if sim_set['algo'] in cmd_algos:
        # Choose parameters / starting point
        if sim_set['algo'] == 'CMDpar':
            [delta0, taui0] = CMD_initpar(sim_params.mod.M, sim_params.L, train_set['start_point'], k = 3)
        else:
            [delta0, taui0] = CMD_initpar(sim_params.mod.M, sim_params.L, train_set['start_point'])
        
        # Initialization
        train_hist = mt.training_history()
        train_hist.filename = fn.pathfile # Set path for checkpoint saves
        # Load results if already trained
        if load_set['train'] == 1:
            train_hist.dict2obj(saveobj.load(train_hist.filename))
        
        # Create graph
        if sim_set['algo'] == 'HyperCMD':
            CMDgraph = mt.HyperCMD_graph(train_params, delta0, taui0, soft = train_set['soft'],
                                multiloss = train_set['multiloss'], binopt = sim_set['binopt'])   
        elif sim_set['algo'] == 'CMDpar':
            CMDgraph = mt.CMDpar_graph(train_params, delta0, taui0, soft = train_set['soft'],
                                multiloss = train_set['multiloss'], binopt = sim_set['binopt'])            
        else:
            CMDgraph = CMD_graph(train_params, delta0, taui0, soft = train_set['soft'],
                                multiloss = train_set['multiloss'], binopt = sim_set['binopt'])
        # Train
        train_hist = train(train_set['Nepoch'], train_hist, CMDgraph.train_inputs, CMDgraph.loss,
                            CMDgraph.params, opt, data_gen, saveobj,
                            it_checkp = train_set['it_checkp'], sv_checkp = train_set['sv_checkp'],
                            sel_best_weights = train_set['sel_bweights'], esteps = train_set['esteps'],
                            lr_dyn = train_set['lr_dyn']['mode'])


    ## TEST (with or without code)
    if codeon:
        fn = mf.filename_module(load_set['fn_sim'], sim_set['path'], sim_set['algo'], sim_set['fn_ext'], sim_set, code_set)
        dataobj = mch.dataset_gen(sim_params, soft = 0, code = 1, G = G, arch = code_set['arch'])
        R_c = G.shape[0] / G.shape[-1]
        sel_crit = 'cber'
    else:
        fn = mf.filename_module(load_set['fn_sim'], sim_set['path'], sim_set['algo'], sim_set['fn_ext'], sim_set)
        dataobj = mch.dataset_gen(sim_params, soft = 0)
        R_c = 1
        sel_crit = 'ber'
    fn.generate_pathfile_MIMO()
    # Save settings
    with open(os.path.join('settings', fn.filename + '.yaml'), 'w') as outfile:
        yaml.dump(set_dict, outfile, default_flow_style = False)
    # Stopping criterion in simulations
    it_max = int(sim_set['sim_prec'] ** -1 * sim_set['Nerr_min'] / (R_c * sim_params.N_batch * sim_params.Nt * np.log2(sim_params.mod.M)))   # maximum number of iterations

    # Load results if already simulated
    perf_meas = mf.performance_measures(sim_set['Nerr_min'], it_max, sel_crit)
    if load_set['sim'] == 1:
        perf_meas.load_results(saveobj.load(fn.pathfile))

    
    
    
    
    if llr_mode != 2:
        if train_set['online'] == 0:
            # MAIN SCRIPT: BER evaluation with random channel and noise realizations
            for ii, snr in enumerate(sim_params.SNR):
                if snr not in perf_meas.SNR:
                    while perf_meas.stop_crit():    # Simulate until 1000 errors or stop after it_max iterations
                        # Generate test data
                        dataobj.sim_par.SNR_range = [snr, snr]
                        dataobj()
                        
                        # Test algorithms: Equalization

                        # 0. ML detector
                        if sim_set['algo'] == 'MAP':
                            s, fr, _ = equ.ml_detector(dataobj.data[0], dataobj.data[1], sim_params.mod.m, res = load_set['prec'])

                        # 1. MMSE solution
                        if sim_set['algo'] == 'MMSE':
                            s, Phi_ee = equ.mmse(dataobj.data[0:3])
                            fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

                        # 2. LS solution
                        if sim_set['algo'] == 'LS':
                            s, Phi_ee = equ.ls_sol(dataobj.data[0:3])
                            fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

                        # 3. Matched Filter
                        if sim_set['algo'] == 'MF':
                            s, Phi_ee = equ.matched_filter(dataobj.data[0:3])
                            fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

                        # 4. MFVI (sequential or parallel)
                        if sim_set['algo'] == 'MFVIseq':
                            s, fr = equ.mfvi(dataobj.data[0:3], sim_params.mod, sim_params.L, seq = 1, binopt = sim_set['binopt'])
                        if sim_set['algo'] == 'MFVIpar':
                            s, fr = equ.mfvi(dataobj.data[0:3], sim_params.mod, sim_params.L, seq = 0, binopt = sim_set['binopt'])

                        # 5. AMP
                        if sim_set['algo'] == 'AMP':
                            s, fr = equ.AMP(dataobj.data[0:3], sim_params.mod, sim_params.L, binopt = sim_set['binopt'])

                        # 6. CMD Unfolded: CMDNet / HyperCMD
                        if sim_set['algo'] in cmd_algos:
                            fr, s = CMDgraph.predict(dataobj.data[0:3])

                        # 7. CMD Detection
                        if sim_set['algo'] == 'CMD_fixed':
                            fr, s = equ.np_CMD(dataobj.data[0:3], sim_params.mod, sim_params.L, delta0, taui0, binopt = sim_set['binopt'])

                        # 8. DetNet loaded
                        if sim_set['algo'] == 'DetNet':
                            a = np.sqrt(3 / (sim_params.mod.M ** 2 - 1))
                            # Method 1
                            HH = mop.batch_dot(np.transpose(dataobj.data[1], (0, 2, 1)), dataobj.data[1]) #* sim_set['Nr']
                            # conversion since m = [-3,-1,3,1] * a in contrast to m = [-3,-1,1,3] in DetNet script
                            yH = mop.batch_dot(dataobj.data[0], dataobj.data[1]) / a #* np.sqrt(sim_set['Nr'])
                            fr, s = detnet.predict([yH, HH])
                            s = s * a
                            #fr = keras.utils.to_categorical(np.argmin(np.abs(s[:, :, np.newaxis] - sim_params.mod.m[np.newaxis, np.newaxis, :]) ** 2, axis = -1), num_classes = sim_params.mod.M)

                        # 9. MMNet / 10. OAMPNet loaded
                        if sim_set['algo'] in ['MMNet', 'OAMPNet']:
                            mmnet_shift = 10 * np.log10(train_params.Nr / train_params.Nt)
                            fr, z, s, x, _ = mmnet.predict([snr - mmnet_shift, snr - mmnet_shift, sim_params.N_batch, dataobj.data[1]])
                            # Extract data from tf graph
                            dataobj.z = z
                            dataobj.x = x
                        
                        # 11. SDR
                        if sim_set['algo'] == 'SDR':
                            fr, s = equ.sdrSolver(dataobj.data[1], dataobj.data[0], sim_params.mod.m)
                        

                        # Decoding
                        if codeon:
                            [bp_out, cr, ur] = com.mimoequ_decoding(fr, dataobj, sim_params, code_set['arch'], H, code_set['dec'], code_set['it'])
                            
                            perf_meas.cber_calc(dataobj.u, ur)
                            perf_meas.cfer_calc(dataobj.u, ur)


                        # Performance evaluation
                        perf_meas.eval(dataobj.z, fr, mod)
                        perf_meas.mse_calc(dataobj.x, s)
                        perf_meas.err_print()
                    
                    # Save only if accuracy high enough after it_max iterations
                    [print_str, sv_flag] = perf_meas.err_saveit(sim_params.SNR[ii], sim_params.EbN0[ii], sim_params.EbN0[ii] - 10 * np.log10(R_c))
                    print('{}, '.format(len(sim_params.SNR) - ii) + print_str)
                    if sv_flag:
                        saveobj.save(fn.pathfile, perf_meas.results()) # Save results to file
        
        




        else:
            # ONLINE TRAINING SCRIPT - Addon for PhD Thesis
            # Online training only with varying channel matrix
            if sim_set['algo'] == 'DNN':
                # 1. Initialize online training DNN, define Keras model
                dnnmimo, loss = MIMODNN(sim_par = sim_params, Nh = train_set['dnnwidth'], NL = train_set['dnndepth'], soft = train_set['soft'])
                # Define metric
                if train_set['soft'] == 1:
                    # Define custom symbol-to-accuracy metric for MSE symbol optimization
                    def sym2acc_metric(constellation):
                        def sym2err_metric(y_true, y_pred):
                            ''' Calculate accuracy metric from symbols using previously stored modulation constellation
                            '''
                            fr_pred = KB.argmin(KB.abs(KB.expand_dims(y_pred, axis = -1) - constellation[np.newaxis, np.newaxis, :]) ** 2, axis = -1)
                            fr_true = KB.argmin(KB.abs(KB.expand_dims(y_true, axis = -1) - constellation[np.newaxis, np.newaxis, :]) ** 2, axis = -1)
                            return 1 - KB.mean(KB.equal(fr_pred, fr_true))
                        return sym2err_metric
                    metrics = [sym2acc_metric(sim_params.mod.m)]
                else:
                    # Default with softmax outputs
                    metrics = [tf.keras.metrics.CategoricalAccuracy()]
                dnnmimo.compile(optimizer = opt, loss = loss, metrics = metrics)


            # Online learning preparations
            Ne_online = train_set['online']
            # Compute number of different channel realizations
            if sim_set['algo'] in cmd_algos:
                online_result_it = np.arange(0, len(train_hist.epoch))[(np.array(train_hist.epoch) % Ne_online == 0) * (np.array(train_hist.epoch) != 0)]
            else:
                online_result_it = np.arange(1,  train_set['Nepoch'] // Ne_online + 1)
            if load_set['sim'] == 1:
                # Restrict to simulations not included in the loaded results
                online_result_it = online_result_it[len(perf_meas.SNR):]
            # Minimum number of validation data iterations
            it_min = 10 # default: 10
            # Calculation and training just for one SNR: Fix SNR!!!
            ii_snr = 0
            snr = sim_params.SNR[ii_snr]
            dataobj.sim_par.SNR_range = [snr, snr]


            for ii, ii_par in enumerate(online_result_it):
                # Iterate through different channel matrices
                if sim_set['algo'] in cmd_algos:
                    # Load weights and corresponding channel matrices and SNRs saved in training history
                    train_hist.sel_weights(CMDgraph.params, ii_par)
                    # delta0, taui0 = train_hist.params[ii_par]
                    Hr, sigma = train_hist.add_params[ii_par - 1]
                    dataobj.Hr = Hr[np.newaxis, :, :]
                    dataobj.sigma = np.array(sigma)[np.newaxis]
                else:
                    # 2. Train low-complex DNN here for one channel realization and same SNR, NOTE: Also other models possible -> replace dnnmimo by model
                    # Shuffle new channel matrix and train with Keras fit function
                    if train_set['lr_dyn']['mode'] == 2 and ii != 0:
                        # Workaround: Reset learning rate schedule per realization by new model compilation
                        opt = opt_sel(opt_name = train_set['opt'], lrs = lrs)
                        dnnmimo.compile(optimizer = opt, loss = loss, metrics = metrics)
                    N_onlinedata = Ne_online * train_set['batch_size']
                    if N_onlinedata > 100000 * 500 or train_set['esteps'] != 0: # 100000 * 500 -> 23 GB for received signal, but 180-250 GB in total needed...
                        # Early stopping, select best weights for online learing DNN, only w.r.t. epochs and thus data generator
                        if train_set['esteps'] == 0:
                            callbacks = []
                        else:
                            callbacks = [tf.keras.callbacks.EarlyStopping(patience = train_set['esteps'], restore_best_weights = train_set['sel_bweights'])]
                        # If online training dataset size to big for system memory, use dataset generator per training iteration
                        # Measure in Gb: round(sys.getsizeof(data_gen.data[0]) / 1024 ** 3, 2)
                        data_gen.online_it = 0      # Reset online dataset generator to create new channel matrix
                        data_gen.gen_online_mimodata()
                        data_gen.online = Ne_online + 1 + 11  # Initialization for validation data + offset of 11 with data generator (reason unkown), so number of iterations needs to be increased
                        # data_gen_mimo = data_gen_mimo2()    # Easy debugging, just comment out and watch data_gen_mimo.it
                        history = dnnmimo.fit(data_gen_mimo(data_gen),
                                            batch_size = None,
                                            epochs = Ne_online,
                                            steps_per_epoch = 1,
                                            validation_data = (data_gen.data_val[0], data_gen.data_val[-1]), # validation_split = 0.2,
                                            # shuffle = False,
                                            # validation_steps = 1,
                                            verbose = train_set['dnnverbose'], # 0, 1, 2 (default), 'auto'
                                            callbacks = callbacks,
                                            )
                    else:
                        # otherwise generate dataset once and train on it
                        data_gen.online_it = 0      # Reset online dataset generator to create new channel matrix
                        data_gen.gen_online_mimodata(Nb = N_onlinedata)
                        history = dnnmimo.fit(x = data_gen.data[0],
                                            y = data_gen.data[-1],
                                            batch_size = train_set['batch_size'],
                                            epochs = 1,
                                            validation_data = (data_gen.data_val[0], data_gen.data_val[-1]), # validation_split = 0.2,
                                            shuffle = False,
                                            validation_steps = 1,
                                            verbose = train_set['dnnverbose'], # 0, 1 (default for interactive), 2, 'auto'
                                            )
                    dataobj.Hr = data_gen.Hr
                    dataobj.sigma = mop.csigma(snr) # Validation SNR can be different from training SNR data_gen.sigma
                
                
                while perf_meas.stop_crit() or len(perf_meas.N_ber) < it_min:    # Simulate until 1000 errors or stop after it_max iterations + simulate minimum number of iterations it_min
                    # Generate test data
                    dataobj.gen_onlineval_mimodata()
                    
                    # Test online learned algorithms for equalization

                    # 6. CMD Unfolded: CMDNet / HyperCMD
                    if sim_set['algo'] in cmd_algos:
                        fr, s = CMDgraph.predict(dataobj.data[0:3])
                    
                    # 13. Online learning low-complex DNN (Fixed channel matrix + SNR)
                    if sim_set['algo'] == 'DNN':
                        # 3. Validate here for different noise realizations
                        pred_target = dnnmimo.predict(dataobj.data[0])
                        if train_set['soft'] == 1:
                            # Soft estimation with MSE
                            s = pred_target
                            fr = equ.konehot(np.argmin(np.abs(s[:, :, np.newaxis] - sim_params.mod.m[np.newaxis, np.newaxis, :]) ** 2, axis = -1), num_classes = sim_params.mod.M)
                        else:
                            # Class probabilites hard output
                            fr = pred_target
                            s = np.sum(fr * sim_params.mod.m[np.newaxis, np.newaxis, :], axis = -1)


                    # Decoding
                    if codeon:
                        [bp_out, cr, ur] = com.mimoequ_decoding(fr, dataobj, sim_params, code_set['arch'], H, code_set['dec'], code_set['it'])
                        
                        perf_meas.cber_calc(dataobj.u, ur)
                        perf_meas.cfer_calc(dataobj.u, ur)


                    # Performance evaluation
                    perf_meas.eval(dataobj.z, fr, mod)
                    perf_meas.mse_calc(dataobj.x, s)
                    perf_meas.err_print()
                    
                # Save only if accuracy high enough after it_max iterations
                [print_str, sv_flag] = perf_meas.err_saveit(sim_params.SNR[ii_snr], sim_params.EbN0[ii_snr], sim_params.EbN0[ii_snr] - 10 * np.log10(R_c))
                print('{}, '.format(len(online_result_it) - ii) + print_str)
                if sv_flag:
                    saveobj.save(fn.pathfile, perf_meas.results(sort = 0)) # Save results to file (sorting not necessary since only one SNR evaluated)

    




    
    elif llr_mode == 2:
        # SPECIAL MODE: LLR OUTPUT EVALUATION
        import matplotlib.pyplot as plt
        import tikzplotlib as tplt

        # Specific LLR simulation settings
        N_llr = 10                      # Number of LLR batches
        snr = sim_params.SNR[0]         # LLR evaluation SNR
        N_bins = 100                    # Histogram resolution, 'auto' (default)
        llr_max = 40                    # Infinity LLRs are clipped to this value in the plot
        llr_thresh = 1000000            # LLR threshold considered to be infinite LLRs

        llr_list = []
        llr2_list = []
        cl_list = []
        dataobj.sim_par.SNR_range = [snr, snr]
        dataobj()
        H = np.repeat(dataobj.data[1][0, :, :][np.newaxis, :, :], dataobj.data[1].shape[0], axis = 0)

        for ii in range(0, N_llr):
            # Generate test data
            dataobj()
            dataobj.data[1] = H
            dataobj.data[0] = com.awgn(mop.batch_dot(H, dataobj.x), dataobj.data[2][:, np.newaxis])

            # Test algorithms
            # Equalization

            # 0. ML detector
            if sim_set['algo'] == 'MAP':
                s, fr, _ = equ.ml_detector(dataobj.data[0], dataobj.data[1], sim_params.mod.m, res = load_set['prec'] )

            # 1. MMSE solution
            if sim_set['algo'] == 'MMSE':
                s, Phi_ee = equ.mmse(dataobj.data[0:3])
                fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

            # 2. LS solution
            if sim_set['algo'] == 'LS':
                s, Phi_ee = equ.ls_sol(dataobj.data[0:3])
                fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

            # 3. Matched Filter
            if sim_set['algo'] == 'MF':
                s, Phi_ee = equ.matched_filter(dataobj.data[0:3])
                fr = equ.lin_det_soft(s, Phi_ee, sim_params.mod.m, sim_params.mod.alpha)

            # 4. MFVI (sequential or parallel)
            if sim_set['algo'] == 'MFVIseq':
                s, fr = equ.mfvi(dataobj.data[0:3], sim_params.mod, sim_params.L, seq = 1, binopt = sim_set['binopt'])
            if sim_set['algo'] == 'MFVIpar':
                s, fr = equ.mfvi(dataobj.data[0:3], sim_params.mod, sim_params.L, seq = 0, binopt = sim_set['binopt'])

            # 5. AMP
            if sim_set['algo'] == 'AMP':
                s, fr = equ.AMP(dataobj.data[0:3], sim_params.mod, sim_params.L, binopt = sim_set['binopt'])

            # 6. CMD Unfolded: CMDNet
            if sim_set['algo'] in cmd_algos:
                fr, s = CMDgraph.predict(dataobj.data[0:3])

            # 7. CMD Detection
            if sim_set['algo'] == 'CMD_fixed':
                fr, s = equ.np_CMD(dataobj.data[0:3], sim_params.mod, sim_params.L, delta0, taui0, binopt = sim_set['binopt'])

            # 8. DetNet loaded
            if sim_set['algo'] == 'DetNet':
                a = np.sqrt(3 / (sim_params.mod.M ** 2 - 1))
                # Method 1
                HH = mop.batch_dot(np.transpose(dataobj.data[1], (0, 2, 1)), dataobj.data[1]) # * sim_set['Nr']
                # Conversion necessary since m = [-3,-1,3,1] * a in contrast to m = [-3,-1,1,3] in DetNet script
                yH = mop.batch_dot(dataobj.data[0], dataobj.data[1]) / a #* np.sqrt(sim_set['Nr'])
                fr, s = detnet.predict([yH, HH])
                s = s * a
                # fr = keras.utils.to_categorical(np.argmin(np.abs(s[:, :, np.newaxis] - sim_params.mod.m[np.newaxis, np.newaxis, :]) ** 2, axis = -1), num_classes = sim_params.mod.M)
            
            # 9. MMNet / 10. OAMPNet loaded
            if sim_set['algo'] == 'MMNet' or sim_set['algo'] == 'OAMPNet':
                mmnet_shift = 10 * np.log10(train_params.Nr / train_params.Nt)
                fr, z, s, x, _ = mmnet.predict([snr - mmnet_shift, snr - mmnet_shift, sim_params.N_batch, dataobj.data[1]])
                # Extract data from tf graph
                dataobj.z = z
                dataobj.x = x
            
            # 11. SDR
            if sim_set['algo'] == 'SDR':
                fr, s = equ.sdrSolver(dataobj.data[1], dataobj.data[0], sim_params.mod.m)
            
            # 12. DetNet loaded [fixed comparison]
            # if sim_set['algo'] == 'DetNet':
            a = np.sqrt(3 / (sim_params.mod.M ** 2 - 1))
            # Method 1
            HH = mop.batch_dot(np.transpose(dataobj.data[1], (0, 2, 1)), dataobj.data[1]) #* sim_set['Nr']
            # Conversion necessary since m = [-3,-1,3,1] * a in contrast to m = [-3,-1,1,3] in DetNet script
            yH = mop.batch_dot(dataobj.data[0], dataobj.data[1]) / a #* np.sqrt(sim_set['Nr'])
            fr2, s2 = detnet_llr.predict([yH, HH])
            s2 = s2 * a
            

            # Decoding
            llr_c2, _ = com.symprob2llr(fr, sim_params.mod.M) # TODO: a-posteriori, but extrinsic information (a-posteriori / a-priori) required?
            llr_c2_2, _ = com.symprob2llr(fr2, sim_params.mod.M)
            #llr_c_perm = com.mimo_decoding(llr_c2, G.shape[-1], sim_params.Nt, sim_params.mod.M, code_set['arch'])
            #llr_c = dataobj.intleav.deinterleave(llr_c_perm)


            # Performance evaluation (LLR histogramm)
            cl_list.append(dataobj.cl[:, 0])
            llr_list.append(llr_c2[:, 0, :])
            llr2_list.append(llr_c2_2[:, 0, :])
            print('it: {}/{}'.format(N_llr - ii, N_llr))


        llr_list = np.array(llr_list).reshape((-1))
        llr2_list = np.array(llr2_list).reshape((-1))
        cl_list = np.array(cl_list).reshape((-1))
        # a, b = np.histogram(llr_list[cl_list == 0])
        print('Simulation ended.')

        ## Create LLR Histograms
        llr_list0 = llr_list[cl_list == 0]
        llr_list0[np.abs(llr_list0) >= llr_thresh] = np.sign(llr_list0[np.abs(llr_list0) >= llr_thresh]) * llr_max
        llr2_list0 = llr2_list[cl_list == 0]
        llr2_list0[np.abs(llr2_list0) >= llr_thresh] = np.sign(llr2_list0[np.abs(llr2_list0) >= llr_thresh]) * llr_max
        hist1 = plt.hist(llr_list0, bins = N_bins, label = sim_set['algo'] + ' +1', weights = np.zeros_like(llr_list0) + 100. / llr_list0.size) # Arguments are passed to np.histogram, bins='auto'
        hist2 = plt.hist(llr2_list0, bins = N_bins, label = 'DetNet +1', weights = np.zeros_like(llr2_list0) + 100. / llr2_list0.size)
        # plt.title("Frequency LLRs x = +1")
        llr_list1 = llr_list[cl_list == 1]
        llr_list1[np.abs(llr_list1) >= llr_thresh] = np.sign(llr_list1[np.abs(llr_list1) >= llr_thresh]) * llr_max
        llr2_list1 = llr2_list[cl_list == 1]
        llr2_list1[np.abs(llr2_list1) >= llr_thresh] = np.sign(llr2_list1[np.abs(llr2_list1) >= llr_thresh]) * llr_max
        hist3 = plt.hist(llr_list1, bins = N_bins, label = sim_set['algo'] + ' -1', weights = np.zeros_like(llr_list1) + 100. / llr_list1.size) # Arguments are passed to np.histogram, bins='auto'
        hist4 = plt.hist(llr2_list1, bins = N_bins, label = 'DetNet -1', weights = np.zeros_like(llr2_list1) + 100. / llr2_list1.size)
        plt.title("Relative Frequency of LLRs")
        plt.legend()
        plt.ylim(0, np.max(hist3[0]) + 1)
        # plt.xlim(10, 20)
        fn_tikz = ''
        tplt.save("plots/MIMOLLRhistogram_" + mod.mod_name + "_{}x{}_{}".format(sim_params.Nt, sim_params.Nr, sim_params.L) + fn_tikz + ".tikz")
        # Statistics
        # It is well known that for Gaussian distributed noise, the LLRs also follow a Gaussian distribution (for a given value + 1 or −1 of the transmit signal) with mean and variance [5]
        # μ_L=4*EsN0 and σ_L^2=2*μ_L=8*EsN0
        channel_pow = np.sum(H[0, 0, :] ** 2)
        print('Channel power: {}'.format(channel_pow))
        print('Condition number: {}'.format(np.linalg.cond(H[0,:,:])))
        print('EsN0: {}'.format(10 ** (sim_params.EbN0[0] / 10) * np.log2(sim_params.mod.M) * channel_pow))
        print('LLR mean 0: {}'.format(np.mean(llr_list0)))
        print('LLR variance 0: {}'.format(np.var(llr_list0)))
        print('LLR mean 1: {}'.format(np.mean(llr_list1)))
        print('LLR variance 1: {}'.format(np.var(llr_list1)))


        ## Save frequencies and histograms in dat-file
        # Bar data in one file
        # plt.hist: Returns values of histogram bins and bin edges
        # TikZ histogram example with bin edges: \addplot+[ybar interval] table{data.dat};
        # Append 0 as last histogram bin for compatibility with TikZ and ybar interval option
        np.savetxt("plots/data/histogram/MIMOLLRhistogram_" + sim_set['algo'] + "_" + mod.mod_name + "_{}x{}_{}".format(sim_params.Nt, sim_params.Nr, sim_params.L) + fn_tikz + ".dat",
                    np.c_[np.append(hist1[0], 0), hist1[1], np.append(hist3[0], 0), hist3[1]],
                    )
        np.savetxt("plots/data/histogram/MIMOLLRhistogram_" + "DetNet" + "_" + mod.mod_name + "_{}x{}_{}".format(sim_params.Nt, sim_params.Nr, sim_params.L) + fn_tikz + ".dat",
                    np.c_[np.append(hist2[0], 0), hist2[1], np.append(hist4[0], 0), hist4[1]],
                    )
        # Frequency saved -> too large for TikZ to process
        np.savetxt("plots/data/histogram/MIMOLLRfreq_+1_" + mod.mod_name + "_{}x{}_{}".format(sim_params.Nt, sim_params.Nr, sim_params.L) + fn_tikz + ".dat",
                    np.c_[llr_list0, llr2_list0],
                    )
        np.savetxt("plots/data/histogram/MIMOLLRfreq_-1_" + mod.mod_name + "_{}x{}_{}".format(sim_params.Nt, sim_params.Nr, sim_params.L) + fn_tikz + ".dat",
                    np.c_[llr_list1, llr2_list1],
                    )
        print('Saved MIMO LLR histograms to "plots/".')

    print('Simulation ended.')
# EOF